{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def1aaf52cf906",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:31:29.956437Z",
     "start_time": "2024-03-05T16:31:29.821091Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import tools.create_masks as masker\n",
    "import tools.image_resizer as resizer\n",
    "import tools.image_annotator as annotator\n",
    "import tools.image_tiles as tiler\n",
    "\n",
    "# pip install opencv-python\n",
    "#apt-get install ffmpeg libsm6 libxext6  -y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c44090-ff78-4919-b822-16f9575acf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00cff09f155b8ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:33:05.753242Z",
     "start_time": "2024-03-05T16:31:30.965330Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "masker.create_masks_for_folder(\"data/source-images\", \"data/source-annotations\", \"data/source-masks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a650f12283db0db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:36:47.238225Z",
     "start_time": "2024-03-05T16:33:20.587616Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "annotator.create_image_annotations_for_folder(\"data/source-images\", \"data/source-annotations\", \"data/source-images-annotated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809ab6c274c66752",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:37:52.566173Z",
     "start_time": "2024-03-05T16:37:11.297646Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "resizer.resize_images_in_folder(\"data/source-images\", \"data/images\", (5000, 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbadcbe3e22e2f4b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "resizer.resize_images_in_folder(\"data/source-masks\", \"data/masks\", (5000, 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7875d464e353e428",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "resizer.resize_images_in_folder(\"data/source-images-annotated\", \"data/images-annotated\", (5000, 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14d3816eeb43fea",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tiler.split_to_tiles(\"data/source-images\", \"data/image-tiles\", \"data/source-masks\", \"data/mask-tiles\", 256, 256 ,\"data/visualized_masks/tiles.csv\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2f1957-84cc-43fe-a6d8-3ad451ab1428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tiler.split_image_into_tiles_with_background(\"data/images/1.jpg\", \"data/test_tiles\", \"data/masks/1.png\", \"data/test_mask_tiles\", 256, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f426850d7010d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429a1ecfd98f26d5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "input_dir = \"data/image-tiles/\"\n",
    "target_dir = \"data/mask-tiles/\"\n",
    "img_size = (256, 256)\n",
    "num_classes = 2\n",
    "batch_size = 32\n",
    "\n",
    "input_img_paths = sorted([os.path.join(input_dir, fname) for fname in os.listdir(input_dir)])\n",
    "target_img_paths = sorted([os.path.join(target_dir, fname) for fname in os.listdir(target_dir)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552d6618c2a2f662",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of samples:\", len(input_img_paths))\n",
    "\n",
    "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
    "    print(input_path, \"|\", target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702aad3800a19a8d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from keras.utils import load_img\n",
    "from PIL import ImageOps\n",
    "\n",
    "# Display input image #7\n",
    "display(Image(filename=input_img_paths[7]))\n",
    "print(input_img_paths[7])\n",
    "# Display auto-contrast version of corresponding target (per-pixel categories)\n",
    "img = ImageOps.autocontrast(load_img(target_img_paths[7]))\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a16aa4d8cef258",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image as pim\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b479452ee0578604",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# img = Image(filename=input_img_paths[7])\n",
    "# img = pim.open(input_img_paths[7], \"r\")\n",
    "# mask = pim.open(target_img_paths[7])\n",
    "img = cv2.imread(input_img_paths[7], cv2.IMREAD_COLOR)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e36e42f44cf0c3d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b309076a765bf6e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "start = 300\n",
    "count = 20\n",
    "imgs = range(start, start + count)\n",
    "imgs = [101, 103, 112, 115, 19, 307]\n",
    "fig, axs = plt.subplots(2, len(imgs), figsize=(10, 3))\n",
    "img = ImageOps.autocontrast(load_img(target_img_paths[7]))\n",
    "x = 0\n",
    "for img_index in imgs:\n",
    "    img = cv2.imread(input_img_paths[img_index], cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # mask = cv2.imread(target_img_paths[img_index], cv2.IMREAD_COLOR)\n",
    "    mask = ImageOps.autocontrast(load_img(target_img_paths[img_index]))\n",
    "    mask = np.array(mask)\n",
    "    axs[0, x].imshow(img)\n",
    "    axs[0, x].get_xaxis().set_visible(False)\n",
    "    axs[0, x].get_yaxis().set_visible(False)\n",
    "    axs[1, x].imshow(mask)\n",
    "    axs[1, x].get_xaxis().set_visible(False)\n",
    "    axs[1, x].get_yaxis().set_visible(False)\n",
    "    x += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf7a1f76ee7e8d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (30,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362bb72e002e7028",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(\"data/images-annotated/1.jpg\", cv2.IMREAD_COLOR)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "mask = cv2.imread(\"data/masks/1.png\")\n",
    "mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "#mask = (((mask * 255)/2)+100).astype(np.uint8)\n",
    "# mask = (mask + 1)/3\n",
    "mask = mask.astype(np.float32)\n",
    "img = img.astype(np.float32)\n",
    "mask[mask == 0] = .5\n",
    "res = img * mask\n",
    "res = res.astype(np.uint8)\n",
    "plt.imshow(res)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd441cdf615d92",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e158df2f9c7d8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from tensorflow import data as tf_data\n",
    "from tensorflow import image as tf_image\n",
    "from tensorflow import io as tf_io\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "    batch_size,\n",
    "    img_size,\n",
    "    input_img_paths,\n",
    "    target_img_paths,\n",
    "    max_dataset_len=None,\n",
    "):\n",
    "    \"\"\"Returns a TF Dataset.\"\"\"\n",
    "\n",
    "    def load_img_masks(input_img_path, target_img_path):\n",
    "        input_img = tf_io.read_file(input_img_path)\n",
    "        input_img = tf_io.decode_png(input_img, channels=3)\n",
    "        input_img = tf_image.resize(input_img, img_size)\n",
    "        input_img = tf_image.convert_image_dtype(input_img, \"float32\")\n",
    "\n",
    "        target_img = tf_io.read_file(target_img_path)\n",
    "        target_img = tf_io.decode_png(target_img, channels=1)\n",
    "        target_img = tf_image.resize(target_img, img_size, method=\"nearest\")\n",
    "        target_img = tf_image.convert_image_dtype(target_img, \"uint8\")\n",
    "\n",
    "        # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
    "        # target_img -= 1\n",
    "        return input_img, target_img\n",
    "\n",
    "    # For faster debugging, limit the size of data\n",
    "    if max_dataset_len:\n",
    "        input_img_paths = input_img_paths[:max_dataset_len]\n",
    "        target_img_paths = target_img_paths[:max_dataset_len]\n",
    "    dataset = tf_data.Dataset.from_tensor_slices((input_img_paths, target_img_paths))\n",
    "    dataset = dataset.map(load_img_masks, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ba4dc0f1f23f53",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "\n",
    "def get_model(img_size, num_classes):\n",
    "    inputs = keras.Input(shape=img_size + (3,))\n",
    "\n",
    "    ### [First half of the network: downsampling inputs] ###\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "    for filters in [64, 128, 256]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    ### [Second half of the network: upsampling inputs] ###\n",
    "\n",
    "    for filters in [256, 128, 64, 32]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        # x = layers.UpSampling2D(2)(x)\n",
    "        x = layers.UpSampling2D(2, interpolation='nearest')(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.UpSampling2D(2, interpolation='nearest')(previous_block_activation)\n",
    "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        # residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "        # residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        \n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Add a per-pixel classification layer\n",
    "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build model\n",
    "model = get_model(img_size, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84327afe75c8b37f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Split our img paths into a training and a validation set\n",
    "val_samples = 1000\n",
    "random.Random(1337).shuffle(input_img_paths)\n",
    "random.Random(1337).shuffle(target_img_paths)\n",
    "train_input_img_paths = input_img_paths[:-val_samples]\n",
    "train_target_img_paths = target_img_paths[:-val_samples]\n",
    "val_input_img_paths = input_img_paths[-val_samples:]\n",
    "val_target_img_paths = target_img_paths[-val_samples:]\n",
    "\n",
    "# Instantiate dataset for each split\n",
    "# Limit input files in `max_dataset_len` for faster epoch training time.\n",
    "# Remove the `max_dataset_len` arg when running with full dataset.\n",
    "train_dataset = get_dataset(\n",
    "    batch_size,\n",
    "    img_size,\n",
    "    train_input_img_paths,\n",
    "    train_target_img_paths,\n",
    "    max_dataset_len=1000,\n",
    ")\n",
    "valid_dataset = get_dataset(\n",
    "    batch_size, img_size, val_input_img_paths, val_target_img_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff1513-a2d1-416b-98e9-1a677c98340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Is GPU available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"CUDA version:\", tf.sysconfig.get_build_info()[\"cuda_version\"])\n",
    "print(\"cuDNN version:\", tf.sysconfig.get_build_info()[\"cudnn_version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c125d605cb528d",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure the model for training.\n",
    "# We use the \"sparse\" version of categorical_crossentropy\n",
    "# because our target data is integers.\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-4), loss=\"sparse_categorical_crossentropy\"\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"oxford_segmentation.tf\", save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch.\n",
    "epochs = 50\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f698c8b9dff0b3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate predictions for all images in the validation set\n",
    "\n",
    "val_dataset = get_dataset(\n",
    "    batch_size, img_size, val_input_img_paths, val_target_img_paths\n",
    ")\n",
    "val_preds = model.predict(val_dataset)\n",
    "\n",
    "\n",
    "def display_mask(i):\n",
    "    \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
    "    mask = np.argmax(val_preds[i], axis=-1)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    img = ImageOps.autocontrast(keras.utils.array_to_img(mask))\n",
    "    display(img)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee2a6c0-de4b-4d04-8957-88fee18f6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1845d52f260759",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Display results for validation image #10\n",
    "\n",
    "\n",
    "# Display mask predicted by our model\n",
    "# display_mask(i)  # Note that the model only sees inputs at 150x150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b263823164edae36",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Display results for validation image #10\n",
    "i = 16\n",
    "\n",
    "# Display input image\n",
    "display(Image(filename=val_input_img_paths[i]))\n",
    "    # Display ground-truth target mask\n",
    "img = ImageOps.autocontrast(load_img(val_target_img_paths[i]))\n",
    "display(img)\n",
    "\n",
    "# Display mask predicted by our model\n",
    "display_mask(i)  # Note that the model only sees inputs at 150x150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46d8999-c663-484f-8a55-17db91b0613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "masker.create_masks_for_folder(\"data/source-images-test\", \"data/source-annotations-test\", \"data/source-masks-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563c014b-30a6-4fb6-af31-d24becf95fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator.create_image_annotations_for_folder(\"data/source-images-test\", \"data/source-annotations-test\", \"data/source-images-annotated-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29da835-ca58-4f11-80d7-72081e4180c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "resizer.resize_images_in_folder(\"data/source-images-test\", \"data/test-images\", (5000, 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eac58b1-6fa7-4fe0-85f7-4a9f61ba1e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resizer.resize_images_in_folder(\"data/source-masks-test\", \"data/test-masks\", (5000, 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac585d87-d9ab-44fe-9f7d-703eb48237a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "resizer.resize_images_in_folder(\"data/source-images-annotated-test\", \"data/images-annotated-test\", (5000, 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287485d8-e19f-4f43-98f2-50a8a04e4c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiler.split_to_tiles(\"data/source-images-test\", \"data/test-tiles\", \"data/source-masks-test\", \"data/test_mask-tiles\", 256, 256 ,\"data/visualized_masks/tile.csv\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c4507-5111-43be-876c-f80cf293d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import ImageOps\n",
    "import keras\n",
    "import numpy as np\n",
    "from tensorflow import data as tf_data\n",
    "from tensorflow import image as tf_image\n",
    "from tensorflow import io as tf_io\n",
    "img_size = (256, 256)\n",
    "num_classes = 2\n",
    "batch_size = 32\n",
    "def get_dataset(\n",
    "    batch_size,\n",
    "    img_size,\n",
    "    input_img_paths,\n",
    "    target_img_paths,\n",
    "    max_dataset_len=None,\n",
    "):\n",
    "    \"\"\"Returns a TF Dataset.\"\"\"\n",
    "\n",
    "    def load_img_masks(input_img_path, target_img_path):\n",
    "        input_img = tf_io.read_file(input_img_path)\n",
    "        input_img = tf_io.decode_png(input_img, channels=3)\n",
    "        input_img = tf_image.resize(input_img, img_size)\n",
    "        input_img = tf_image.convert_image_dtype(input_img, \"float32\")\n",
    "\n",
    "        target_img = tf_io.read_file(target_img_path)\n",
    "        target_img = tf_io.decode_png(target_img, channels=1)\n",
    "        target_img = tf_image.resize(target_img, img_size, method=\"nearest\")\n",
    "        target_img = tf_image.convert_image_dtype(target_img, \"uint8\")\n",
    "\n",
    "        # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
    "        # target_img -= 1\n",
    "        return input_img, target_img\n",
    "\n",
    "    # For faster debugging, limit the size of data\n",
    "    if max_dataset_len:\n",
    "        input_img_paths = input_img_paths[:max_dataset_len]\n",
    "        target_img_paths = target_img_paths[:max_dataset_len]\n",
    "    dataset = tf_data.Dataset.from_tensor_slices((input_img_paths, target_img_paths))\n",
    "    dataset = dataset.map(load_img_masks, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "    return dataset.batch(batch_size)\n",
    "model = tf.keras.models.load_model('oxford_segmentation.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dd5c66-0aa8-4600-9ab1-1832c40af81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tools.visualise as visualizer\n",
    "input_img_paths = sorted([os.path.join(\"data/test_tiles\", fname) for fname in os.listdir(\"data/test_tiles\")])\n",
    "target_img_paths = sorted([os.path.join(\"data/test_mask_tiles\", fname) for fname in os.listdir(\"data/test_mask_tiles\")])\n",
    "input_img_paths = sorted(input_img_paths, key=lambda x: [int(i) if i.isdigit() else i for i in re.split('(\\d+)', x)])\n",
    "target_img_paths = sorted(target_img_paths, key=lambda x: [int(i) if i.isdigit() else i for i in re.split('(\\d+)', x)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319c12a7-5818-46e5-bf3d-b2df4083504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_dataset = get_dataset(\n",
    "#    16, img_size, input_img_paths,target_img_paths\n",
    "#)\n",
    "\n",
    "#test_preds = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0499c739-3d45-4e55-adb1-e67f68a5693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def find_images_with_prefix(base_path, prefix):\n",
    "    image_set = set()\n",
    "    for fname in os.listdir(base_path):\n",
    "        if fname.startswith(prefix):\n",
    "            image_set.add(os.path.join(base_path, fname))\n",
    "    sorted_images = sorted(image_set, key=lambda x: [int(i) if i.isdigit() else i for i in re.split('(\\d+)', x)])\n",
    "    return sorted_images\n",
    "    \n",
    "base_path = \"data/test-tiles\"\n",
    "base_mask_path = \"data/test_mask-tiles\"\n",
    "prefix_45 = \"45.\"\n",
    "prefix_79 = \"79.\"\n",
    "prefix_43 = \"43.\"\n",
    "prefix_91 = \"91.\"\n",
    "input_img_paths_45 = find_images_with_prefix(base_path, prefix_45)\n",
    "target_img_paths_45 = find_images_with_prefix(base_mask_path, prefix_45)\n",
    "input_img_paths_79 = find_images_with_prefix(base_path, prefix_79)\n",
    "target_img_paths_79 = find_images_with_prefix(base_mask_path, prefix_79)\n",
    "input_img_paths_43 = find_images_with_prefix(base_path, prefix_43)\n",
    "target_img_paths_43 = find_images_with_prefix(base_mask_path, prefix_43)\n",
    "input_img_paths_91 = find_images_with_prefix(base_path, prefix_91)\n",
    "target_img_paths_91 = find_images_with_prefix(base_mask_path, prefix_91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e6f55f-4cb4-498e-ac73-61d0657d54cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_45 = get_dataset(\n",
    "    16, img_size, input_img_paths_45,target_img_paths_45\n",
    ")\n",
    "\n",
    "test_preds_45 = model.predict(test_dataset_45)\n",
    "\n",
    "test_dataset_79 = get_dataset(\n",
    "    16, img_size, input_img_paths_79,target_img_paths_79\n",
    ")\n",
    "\n",
    "test_preds_79 = model.predict(test_dataset_79)\n",
    "\n",
    "test_dataset_43 = get_dataset(\n",
    "    16, img_size, input_img_paths_43, target_img_paths_43\n",
    ")\n",
    "\n",
    "test_preds_43 = model.predict(test_dataset_43)\n",
    "\n",
    "test_dataset_91 = get_dataset(\n",
    "    16, img_size, input_img_paths_91,target_img_paths_91\n",
    ")\n",
    "\n",
    "test_preds_91 = model.predict(test_dataset_91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fdade9-e1d1-4c45-87b4-3beac0162691",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_list = []\n",
    "def generate_pred_list(i):\n",
    "    mask = np.argmax(test_preds_45[i], axis=-1)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    img = ImageOps.autocontrast(keras.utils.array_to_img(mask))\n",
    "    test_pred_list.append(img)\n",
    "\n",
    "for i in range(len(test_preds_45)):\n",
    "    generate_pred_list(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faac0df-cec5-494c-8ee8-5177ab8bcb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_rect_45 = visualizer.overlay_gd_and_rectangles(mask_dir_path=\"data/source-masks-test/\", file_path=\"data/visualized_masks/tile.csv\", image_number=45, dir_path=\"data/source-images-test/\", save_path=\"data/visualized_masks/rectangles_\" ,mask_save_path=\"data/visualized_masks/merged_rectangles_with_mask_\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e0d66-2e5b-41e2-8158-94adb24be64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_45 = visualizer.merge_prediction_csv(\"data/visualized_masks/tile.csv\", test_pred_list, input_img_paths_45, \"data/source-images-test/\", 45, \"data/visualized_masks/prediction_mask_\", \"data/source-images-test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd84a6dc-9d7b-466a-b94b-7510912d28d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mask_45 = visualizer.final_mask_overlay(\"data/visualized_masks/prediction_mask_\",  45,   \"data/visualized_masks/merged_rectangles_with_mask_\", \"data/visualized_masks/final_mask_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2102959-dff7-4594-9cde-ae3ce2e5aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_list = []\n",
    "def generate_pred_list(i):\n",
    "    mask = np.argmax(test_preds_79[i], axis=-1)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    img = ImageOps.autocontrast(keras.utils.array_to_img(mask))\n",
    "    test_pred_list.append(img)\n",
    "\n",
    "for i in range(len(test_preds_79)):\n",
    "    generate_pred_list(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc19136-76af-43a6-b2ef-61cb5805f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_rect_79 = visualizer.overlay_gd_and_rectangles(\"data/source-masks-test/\", \"data/visualized_masks/tile.csv\", 79, \"data/source-images-test/\", \"data/visualized_masks/rectangles_\" ,\"data/visualized_masks/merged_rectangles_with_mask_\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf4619-7ba6-4e39-8d5e-72288edd3257",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_79 = visualizer.merge_prediction_csv(\"data/visualized_masks/tile.csv\", test_pred_list, input_img_paths_79, \"data/source-images-test/\", 79, \"data/visualized_masks/prediction_mask_\", \"data/source-images-test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61f22d9-9517-4c57-baf2-51bf5a20d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mask_79 = visualizer.final_mask_overlay(\"data/visualized_masks/prediction_mask_\", 79, \"data/visualized_masks/merged_rectangles_with_mask_\",  \"data/visualized_masks/final_mask_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b7e6a-0f52-40a5-a344-c56246aafb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_list = []\n",
    "def generate_pred_list(i):\n",
    "    mask = np.argmax(test_preds_43[i], axis=-1)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    img = ImageOps.autocontrast(keras.utils.array_to_img(mask))\n",
    "    test_pred_list.append(img)\n",
    "\n",
    "for i in range(len(test_preds_43)):\n",
    "    generate_pred_list(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03dfaf-9233-4e72-8973-5767d5351489",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_rect_43 = visualizer.overlay_gd_and_rectangles(\"data/source-masks-test/\", \"data/visualized_masks/tile.csv\", 43, \"data/source-images-test/\", \"data/visualized_masks/rectangles_\" ,\"data/visualized_masks/merged_rectangles_with_mask_\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27997282-0249-466e-b079-9ddb303fb64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_43 = visualizer.merge_prediction_csv(\"data/visualized_masks/tile.csv\", test_pred_list, input_img_paths_43, \"data/source-images-test/\", 43, \"data/visualized_masks/prediction_mask_\", \"data/source-images-test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7199a855-0530-4c09-b536-2a0628d21d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mask_43 = visualizer.final_mask_overlay(\"data/visualized_masks/prediction_mask_\", 43,\"data/visualized_masks/merged_rectangles_with_mask_\",  \"data/visualized_masks/final_mask_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db721e73-a000-44ed-9ab6-e08e0fcd03aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_list = []\n",
    "def generate_pred_list(i):\n",
    "    mask = np.argmax(test_preds_91[i], axis=-1)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    img = ImageOps.autocontrast(keras.utils.array_to_img(mask))\n",
    "    test_pred_list.append(img)\n",
    "\n",
    "for i in range(len(test_preds_91)):\n",
    "    generate_pred_list(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd491f-2cc7-4dbe-8457-d19af9e9189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_rect_91 = visualizer.overlay_gd_and_rectangles(\"data/source-masks-test/\", \"data/visualized_masks/tile.csv\", 91, \"data/source-images-test/\", \"data/visualized_masks/rectangles_\" ,\"data/visualized_masks/merged_rectangles_with_mask_\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd988d08-e285-45e9-ac81-390eaed3749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_91 = visualizer.merge_prediction_csv(\"data/visualized_masks/tile.csv\", test_pred_list, input_img_paths_91, \"data/source-images-test/\", 91, \"data/visualized_masks/prediction_mask_\", \"data/source-images-test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9b068f-d968-48a9-9f3e-762d1951b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mask_91 = visualizer.final_mask_overlay(\"data/visualized_masks/prediction_mask_\", 91,\"data/visualized_masks/merged_rectangles_with_mask_\",  \"data/visualized_masks/final_mask_\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
